{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-02T17:14:27.167403Z","iopub.execute_input":"2025-08-02T17:14:27.167642Z","iopub.status.idle":"2025-08-02T17:14:27.988859Z","shell.execute_reply.started":"2025-08-02T17:14:27.167624Z","shell.execute_reply":"2025-08-02T17:14:27.988235Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import gensim.downloader as api\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\nfrom gensim.models import Word2Vec, FastText\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Download required NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Part 1: Word2Vec Exploration with Pretrained Model\ndef part1_word2vec_exploration():\n    # Load pretrained Word2Vec model\n    print(\"Loading pretrained Word2Vec model...\")\n    w2v_model = api.load('word2vec-google-news-300')\n    \n    # Task 1: Find similar words for 5 chosen words\n    words = ['computer', 'love', 'car', 'school', 'music']\n    print(\"\\nSimilar words for selected words:\")\n    for word in words:\n        try:\n            similar_words = w2v_model.most_similar(word, topn=5)\n            print(f\"\\nWord: {word}\")\n            print(\"Similar words:\", [(w, round(score, 4)) for w, score in similar_words])\n        except KeyError:\n            print(f\"\\nWord: {word} not in vocabulary\")\n    \n    # Task 2: Test vector arithmetic (similar to king - man + woman ~= queen)\n    analogies = [\n        ('king', 'man', 'woman', 'queen'),\n        ('paris', 'france', 'italy', 'rome'),\n        ('big', 'bigger', 'small', 'smaller')\n    ]\n    print(\"\\nTesting vector arithmetic analogies:\")\n    for w1, w2, w3, expected in analogies:\n        try:\n            result = w2v_model.most_similar(positive=[w1, w3], negative=[w2], topn=1)\n            print(f\"{w1} - {w2} + {w3} ~= {result[0][0]} (expected: {expected})\")\n        except KeyError as e:\n            print(f\"Word not in vocabulary: {e}\")\n\n# Part 2: Movie Review Sentiment Classifier\ndef clean_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove punctuation and special characters\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Tokenize\n    tokens = word_tokenize(text)\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token not in stop_words]\n    return tokens\n\ndef get_document_vector(tokens, model, model_type='w2v'):\n    vectors = []\n    for token in tokens:\n        try:\n            if model_type == 'w2v' or model_type == 'fasttext':\n                vectors.append(model[token])\n            else:  # For gensim Word2Vec/FastText\n                vectors.append(model.wv[token])\n        except KeyError:\n            continue\n    if vectors:\n        return np.mean(vectors, axis=0)\n    return np.zeros(300)  # Return zero vector if no valid tokens\n\ndef train_and_evaluate(X, y, model_name):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    clf = LogisticRegression(max_iter=1000)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    \n    return {\n        'Model': model_name,\n        'Accuracy': accuracy_score(y_test, y_pred),\n        'Precision': precision_score(y_test, y_pred),\n        'Recall': recall_score(y_test, y_pred),\n        'F1-Score': f1_score(y_test, y_pred)\n    }\n\ndef part2_sentiment_classifier():\n    # Load IMDB dataset\n    print(\"\\nLoading IMDB dataset...\")\n    df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n    df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n    \n    # Task 1: Text EDA\n    df['review_length'] = df['review'].apply(lambda x: len(word_tokenize(x)))\n    print(\"\\nEDA - Review Length Statistics:\")\n    print(df['review_length'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.histplot(df['review_length'], bins=50)\n    plt.title('Distribution of Review Lengths')\n    plt.xlabel('Number of Words')\n    plt.ylabel('Frequency')\n    plt.savefig('review_length_distribution.png')\n    plt.close()\n    \n    # Task 2: Clean text\n    print(\"\\nCleaning text data...\")\n    df['tokens'] = df['review'].apply(clean_text)\n    \n    # Task 3: Train models with different embeddings\n    results = []\n    \n    # 3.1 Pretrained Word2Vec\n    print(\"Loading pretrained Word2Vec model for sentiment analysis...\")\n    w2v_model = api.load('word2vec-google-news-300')\n    X_w2v = np.array([get_document_vector(tokens, w2v_model, 'w2v') for tokens in df['tokens']])\n    results.append(train_and_evaluate(X_w2v, df['sentiment'], 'Pretrained Word2Vec'))\n    \n    # 3.2 Custom Skip-gram\n    print(\"Training custom Skip-gram model...\")\n    skipgram_model = Word2Vec(sentences=df['tokens'], vector_size=300, window=5, min_count=5, sg=1, workers=4)\n    X_skipgram = np.array([get_document_vector(tokens, skipgram_model, 'custom') for tokens in df['tokens']])\n    results.append(train_and_evaluate(X_skipgram, df['sentiment'], 'Custom Skip-gram'))\n    \n    # 3.3 Custom CBOW\n    print(\"Training custom CBOW model...\")\n    cbow_model = Word2Vec(sentences=df['tokens'], vector_size=300, window=5, min_count=5, sg=0, workers=4)\n    X_cbow = np.array([get_document_vector(tokens, cbow_model, 'custom') for tokens in df['tokens']])\n    results.append(train_and_evaluate(X_cbow, df['sentiment'], 'Custom CBOW'))\n    \n    # 3.4 Custom FastText\n    print(\"Training custom FastText model...\")\n    fasttext_model = FastText(sentences=df['tokens'], vector_size=300, window=5, min_count=5, workers=4)\n    X_fasttext = np.array([get_document_vector(tokens, fasttext_model, 'custom') for tokens in df['tokens']])\n    results.append(train_and_evaluate(X_fasttext, df['sentiment'], 'Custom FastText'))\n    \n    # Task 4: Tabulate results\n    results_df = pd.DataFrame(results)\n    print(\"\\nModel Performance Statistics:\")\n    print(results_df)\n    results_df.to_csv('model_performance.csv', index=False)\n\nif __name__ == \"__main__\":\n    part1_word2vec_exploration()\n    part2_sentiment_classifier()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T17:46:14.974480Z","iopub.execute_input":"2025-08-02T17:46:14.974778Z","iopub.status.idle":"2025-08-02T17:58:28.591494Z","shell.execute_reply.started":"2025-08-02T17:46:14.974759Z","shell.execute_reply":"2025-08-02T17:58:28.590560Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Loading pretrained Word2Vec model...\n\nSimilar words for selected words:\n\nWord: computer\nSimilar words: [('computers', 0.7979), ('laptop', 0.664), ('laptop_computer', 0.6549), ('Computer', 0.6473), ('com_puter', 0.6082)]\n\nWord: love\nSimilar words: [('loved', 0.6908), ('adore', 0.6817), ('loves', 0.6619), ('passion', 0.6101), ('hate', 0.6004)]\n\nWord: car\nSimilar words: [('vehicle', 0.7821), ('cars', 0.7424), ('SUV', 0.7161), ('minivan', 0.6907), ('truck', 0.6736)]\n\nWord: school\nSimilar words: [('elementary', 0.7869), ('schools', 0.7412), ('shool', 0.6692), ('elementary_schools', 0.6597), ('kindergarten', 0.653)]\n\nWord: music\nSimilar words: [('classical_music', 0.7198), ('jazz', 0.6835), ('Music', 0.6596), ('Without_Donny_Kirshner', 0.6416), ('songs', 0.6396)]\n\nTesting vector arithmetic analogies:\nking - man + woman ~= queen (expected: queen)\nparis - france + italy ~= lohan (expected: rome)\nbig - bigger + small ~= large (expected: smaller)\n\nLoading IMDB dataset...\n\nEDA - Review Length Statistics:\ncount    50000.000000\nmean       279.483720\nstd        207.949849\nmin          8.000000\n25%        151.000000\n50%        209.000000\n75%        340.000000\nmax       2911.000000\nName: review_length, dtype: float64\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"name":"stdout","text":"\nCleaning text data...\nLoading pretrained Word2Vec model for sentiment analysis...\nTraining custom Skip-gram model...\nTraining custom CBOW model...\nTraining custom FastText model...\n\nModel Performance Statistics:\n                 Model  Accuracy  Precision    Recall  F1-Score\n0  Pretrained Word2Vec    0.8529   0.853828  0.854336  0.854082\n1     Custom Skip-gram    0.8850   0.883002  0.889661  0.886319\n2          Custom CBOW    0.8714   0.867149  0.879540  0.873300\n3      Custom FastText    0.8575   0.854036  0.865053  0.859509\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}